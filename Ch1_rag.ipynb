{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516fac83-b0fa-49f3-a5a2-1ab8d8f95ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934b10c6-8d21-4f80-8eac-b233a6e9f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai==1.40.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e8046f-f92a-4c64-9354-53d3167b291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt','r') as key:\n",
    "    API_KEY = key.readline().strip()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY # we define the env varibale to be accessible for all scripts not just the currently one.\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5709f52-5c29-4fa0-8c7c-b31169084e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "gptmodel='gpt-4o'\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb639d5-2eb9-472d-8035-6a197eaa9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which creates a prompt\n",
    "def call_llm_with_full_txt(txt):\n",
    "    txt_input= textwrap.fill(txt,width=30)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{txt_input}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model= gptmodel,\n",
    "            messages=[{'role':'system', 'content':\"You are an expert Natural Language Processing exercise expert.\"},\n",
    "                  {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1)\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e :\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee32c7d7-783b-4c41-a414-ea8247d9afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_response(response):\n",
    "    wrapper = textwrap.TextWrapper(width=80)\n",
    "    wrap_res = wrapper.fill(text=response)\n",
    "    print(\"Response:\")\n",
    "    print(\"---------\")\n",
    "    print(wrap_res)\n",
    "    print(\"-------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b643539-bed2-4106-ab85-d4c5b1ebbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd97d479-675a-4e79-b024-8ce844ac79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records_join = ''.join(db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763043ad-9a2b-4a08-a01c-15f2420a2ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.This component merges the outputs from the language model and the retrieval system.It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.When a query or prompt is received, the system first processes it to understand the requirement or the context.Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.The retrieved documents are then fed into the language model.In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.The language model, now augmented with direct access to retrieved information, generates a response.This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.This allows them to remain current with the latest knowledge and trends without needing frequent retraining.With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.A RAG vector store is a database or dataset that contains vectorized data points.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_records_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2ab7c1-9fad-490f-b66a-e6b13da69999",
   "metadata": {},
   "outputs": [],
   "source": [
    "wraped_db_records = textwrap.fill(db_records_join, width=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0420f06b-fdd4-468e-b6e7-d4e21f32ca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Retrieval Augmented Generation\\n(RAG) represents a\\nsophisticated hybrid approach\\nin the field of artificial\\nintelligence, particularly\\nwithin the realm of natural\\nlanguage processing (NLP).It\\ninnovatively combines the\\ncapabilities of neural\\nnetwork-based language models\\nwith retrieval systems to\\nenhance the generation of\\ntext, making it more accurate,\\ninformative, and contextually\\nrelevant.This methodology\\nleverages the strengths of\\nboth generative and retrieval\\narchitectures to tackle\\ncomplex tasks that require not\\nonly linguistic fluency but\\nalso factual correctness and\\ndepth of knowledge.At the core\\nof Retrieval Augmented\\nGeneration (RAG) is a\\ngenerative model, typically a\\ntransformer-based neural\\nnetwork, similar to those used\\nin models like GPT (Generative\\nPre-trained Transformer) or\\nBERT (Bidirectional Encoder\\nRepresentations from\\nTransformers).This component\\nis responsible for producing\\ncoherent and contextually\\nappropriate language outputs\\nbased on a mixture of input\\nprompts and additional\\ninformation fetched by the\\nretrieval\\ncomponent.Complementing the\\nlanguage model is the\\nretrieval system, which is\\nusually built on a database of\\ndocuments or a corpus of\\ntexts.This system uses\\ntechniques from information\\nretrieval to find and fetch\\ndocuments that are relevant to\\nthe input query or prompt.The\\nmechanism of relevance\\ndetermination can range from\\nsimple keyword matching to\\nmore complex semantic search\\nalgorithms which interpret the\\nmeaning behind the query to\\nfind the best matches.This\\ncomponent merges the outputs\\nfrom the language model and\\nthe retrieval system.It\\neffectively synthesizes the\\nraw data fetched by the\\nretrieval system into the\\ngenerative process of the\\nlanguage model.The integrator\\nensures that the information\\nfrom the retrieval system is\\nseamlessly incorporated into\\nthe final text output,\\nenhancing the model's ability\\nto generate responses that are\\nnot only fluent and\\ngrammatically correct but also\\nrich in factual details and\\ncontext-specific nuances.When\\na query or prompt is received,\\nthe system first processes it\\nto understand the requirement\\nor the context.Based on the\\nprocessed query, the retrieval\\nsystem searches through its\\ndatabase to find relevant\\ndocuments or information\\nsnippets.This retrieval is\\nguided by the similarity of\\ncontent in the documents to\\nthe query, which can be\\ndetermined through various\\ntechniques like vector\\nembeddings or semantic\\nsimilarity measures.The\\nretrieved documents are then\\nfed into the language model.In\\nsome implementations, this\\nintegration happens at the\\ntoken level, where the model\\ncan access and incorporate\\nspecific pieces of information\\nfrom the retrieved texts\\ndynamically as it generates\\neach part of the response.The\\nlanguage model, now augmented\\nwith direct access to\\nretrieved information,\\ngenerates a response.This\\nresponse is not only\\ninfluenced by the training of\\nthe model but also by the\\nspecific facts and details\\ncontained in the retrieved\\ndocuments, making it more\\ntailored and accurate.By\\ndirectly incorporating\\ninformation from external\\nsources, Retrieval Augmented\\nGeneration (RAG) models can\\nproduce responses that are\\nmore factual and relevant to\\nthe given query.This is\\nparticularly useful in domains\\nlike medical advice, technical\\nsupport, and other areas where\\nprecision and up-to-date\\nknowledge are\\ncrucial.Retrieval Augmented\\nGeneration (RAG) systems can\\ndynamically adapt to new\\ninformation since they\\nretrieve data in real-time\\nfrom their databases.This\\nallows them to remain current\\nwith the latest knowledge and\\ntrends without needing\\nfrequent retraining.With\\naccess to a wide range of\\ndocuments, Retrieval Augmented\\nGeneration (RAG) systems can\\nprovide detailed and nuanced\\nanswers that a standalone\\nlanguage model might not be\\ncapable of generating based\\nsolely on its pre-trained\\nknowledge.While Retrieval\\nAugmented Generation (RAG)\\noffers substantial benefits,\\nit also comes with its\\nchallenges.These include the\\ncomplexity of integrating\\nretrieval and generation\\nsystems, the computational\\noverhead associated with real-\\ntime data retrieval, and the\\nneed for maintaining a large,\\nup-to-date, and high-quality\\ndatabase of retrievable\\ntexts.Furthermore, ensuring\\nthe relevance and accuracy of\\nthe retrieved information\\nremains a significant\\nchallenge, as does managing\\nthe potential for introducing\\nbiases or errors from the\\nexternal sources.In summary,\\nRetrieval Augmented Generation\\nrepresents a significant\\nadvancement in the field of\\nartificial intelligence,\\nmerging the best of retrieval-\\nbased and generative\\ntechnologies to create systems\\nthat not only understand and\\ngenerate natural language but\\nalso deeply comprehend and\\nutilize the vast amounts of\\ninformation available in\\ntextual form.A RAG vector\\nstore is a database or dataset\\nthat contains vectorized data\\npoints.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wraped_db_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb33ccdd-e91d-4e13-a7df-cfcbfc6a0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'define a rag store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b1ab9b-3755-4595-9f1b-dd23120b1fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "A \"rag store\" typically refers to a shop or business that deals in the buying\n",
      "and selling of rags or used clothing. Historically, rag stores were places where\n",
      "people could sell their old clothes or textiles, which would then be sorted,\n",
      "cleaned, and either resold as second-hand clothing or repurposed for other uses.\n",
      "These stores played a significant role in the recycling and reuse of textiles\n",
      "before the advent of modern recycling systems.  In more detail, a rag store\n",
      "might operate in the following ways:  1. **Collection**: The store collects used\n",
      "clothing and textiles from individuals, donations, or other sources. This could\n",
      "include everything from everyday clothing to linens and other fabric items.  2.\n",
      "**Sorting**: Once collected, the items are sorted based on their condition,\n",
      "type, and potential for resale or repurposing. Clothing in good condition might\n",
      "be cleaned and sold as second-hand apparel, while items that are too worn might\n",
      "be categorized as rags.  3. **Resale**: Clothing and textiles that are still\n",
      "wearable are often sold at a lower price than new items, making them accessible\n",
      "to a broader range of customers. This aspect of a rag store supports sustainable\n",
      "fashion by extending the life cycle of clothing.  4. **Repurposing**: Items that\n",
      "cannot be sold as clothing might be repurposed. For example, they could be cut\n",
      "into cleaning rags, used in industrial applications, or processed into fibers\n",
      "for new textile products.  5. **Recycling**: Some rag stores may also engage in\n",
      "textile recycling, where unusable textiles are broken down into raw materials\n",
      "that can be used to create new products, thus contributing to waste reduction\n",
      "and environmental sustainability.  Overall, rag stores are an important part of\n",
      "the circular economy, promoting the reuse and recycling of textiles and reducing\n",
      "the environmental impact of clothing waste.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_response = call_llm_with_full_txt(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1002d-73ed-44c8-bd31-a388c19cccfe",
   "metadata": {},
   "source": [
    "### Retrieval metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195e35c-6bcb-4aa4-990e-b24d2382b83b",
   "metadata": {},
   "source": [
    "## Cosine Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4d5a80-c48a-4762-8d6a-e6e48443b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1,text2):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', # ignores common english words\n",
    "                                 use_idf=True, # enables idf weighting\n",
    "                                 norm = 'l2', # applies L2 normalization to each output vector\n",
    "                                 ngram_range=(1,2), # consider both single and two-word combinations\n",
    "                                 sublinear_tf= True, # applies log term frequency scaling (ex. replace tf with 1 + log(tf).)\n",
    "                                 analyzer='word' # analyzes text at the word level\n",
    "                                )\n",
    "    tfidf = vectorizer.fit_transform([text1,text2]) # converts the query and record to vectors.\n",
    "    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921ca3b-e4b0-46f8-8c01-d67f59bbc558",
   "metadata": {},
   "source": [
    "## Enhanced Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5faa3e2b-3123-491d-b638-790babbfeae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /Users/hivagheisari/Desktop/sample_project/env/lib/python3.12/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\" --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad402290-15c3-4fa0-a0f5-43cf3f4724d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a00be48a-80c2-4e28-bad7-cb1f54fe48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hivagheisari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        if token.is_stop or token.is_punct:\n",
    "            continue\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words\n",
    "\n",
    "def expand_with_synonyms(words):\n",
    "    expanded_words = words.copy()\n",
    "    for word in words:\n",
    "        expanded_words.extend(get_synonyms(word))\n",
    "    return expanded_words\n",
    "\n",
    "def calculate_enhanced_similarity(text1, text2):\n",
    "    # Preprocess and tokenize texts\n",
    "    words1 = preprocess_text(text1)\n",
    "    words2 = preprocess_text(text2)\n",
    "\n",
    "    # Expand with synonyms\n",
    "    words1_expanded = expand_with_synonyms(words1)\n",
    "    words2_expanded = expand_with_synonyms(words2)\n",
    "\n",
    "    # Count word frequencies\n",
    "    freq1 = Counter(words1_expanded)\n",
    "    freq2 = Counter(words2_expanded)\n",
    "\n",
    "    # Create a set of all unique words\n",
    "    unique_words = set(freq1.keys()).union(set(freq2.keys()))\n",
    "\n",
    "    # Create frequency vectors\n",
    "    vector1 = [freq1[word] for word in unique_words]\n",
    "    vector2 = [freq2[word] for word in unique_words]\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760da414-ff73-4e5d-b06c-de7842a4ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_keymatch(query,records):\n",
    "    best_score= 0\n",
    "    best_record= None\n",
    "    query_kw= set(query.lower().split())\n",
    "    for record in records:\n",
    "        record_kw= set(record.lower().split())\n",
    "        common_kw= query_kw.intersection(record_kw)\n",
    "        current_score= len(common_kw)\n",
    "        if current_score > best_score:\n",
    "            best_score= current_score\n",
    "            best_record= record\n",
    "    return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e02f1bc0-bf35-4466-8824-67a4222b1469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Keyword Score: 3\n",
      "Response:\n",
      "---------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_keyword_score, best_matching_record = find_best_keymatch(query, db_records)\n",
    "\n",
    "print(f\"Best Keyword Score: {best_keyword_score}\")\n",
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be5fabc8-a0d9-4d62-8836-a91f75518b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w= \"A RAG vector store is a database or dataset that contains vectorized data points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b4e6842-2650-45ae-8423-7543b7e4213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w in db_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8d5d8-a674-4b87-b57e-372b074ad0a4",
   "metadata": {},
   "source": [
    "### Cosine Similarity metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1359a7c4-3569-4063-a885-851de044d9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12631460871586422"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_cosine_similarity(query, best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48951c5d-871f-4b4a-a663-16623d237209",
   "metadata": {},
   "source": [
    "### Enhanced Similarity metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b323ccf4-1920-46d9-af6d-ab5879c9d791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.641582812483307"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_enhanced_similarity(query, best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bb848-d834-4368-9ac1-3065b8315a38",
   "metadata": {},
   "source": [
    "### Augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4daba704-a103-4865-8c28-4ccef366d2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'define a rag store: A RAG vector store is a database or dataset that contains vectorized data points.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_input = query+ \": \"+ best_matching_record\n",
    "augmented_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cca2663-0470-4e96-a0b2-f7d606195fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050f1a5-7bc4-43fc-962b-36d500678944",
   "metadata": {},
   "source": [
    "### Generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b50a4e5b-3b6e-4860-b329-00e762ee2c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "Certainly! Let's break down the concept of a RAG vector store and understand its\n",
      "components and purpose.  ### RAG Vector Store  **RAG** stands for **Retrieval-\n",
      "Augmented Generation**. It is a framework used in natural language processing\n",
      "(NLP) that combines the strengths of information retrieval and text generation.\n",
      "The idea is to enhance the generation of text by retrieving relevant information\n",
      "from a large dataset, which can then be used to produce more accurate and\n",
      "contextually relevant responses.  ### Vector Store  A **vector store** is a\n",
      "specialized database or dataset designed to store and manage vectorized data\n",
      "points. In the context of machine learning and NLP, data is often represented as\n",
      "vectors, which are numerical representations of information. These vectors can\n",
      "capture semantic meanings, relationships, and other features of the data.  ####\n",
      "Key Characteristics of a Vector Store:  1. **Vectorization**: Data points are\n",
      "transformed into vectors using techniques like word embeddings (e.g., Word2Vec,\n",
      "GloVe), sentence embeddings, or other feature extraction methods. This\n",
      "transformation allows the data to be represented in a continuous, multi-\n",
      "dimensional space.  2. **Similarity Search**: Vector stores are optimized for\n",
      "similarity search operations. This means they can efficiently find vectors that\n",
      "are similar to a given query vector. This is crucial for tasks like information\n",
      "retrieval, where you want to find documents or data points that are most\n",
      "relevant to a user's query.  3. **Scalability**: Vector stores are designed to\n",
      "handle large volumes of data, making them suitable for applications that require\n",
      "processing and retrieving information from extensive datasets.  4. **Integration\n",
      "with Machine Learning Models**: Vector stores can be integrated with machine\n",
      "learning models to enhance their performance. For example, in a RAG framework, a\n",
      "vector store can be used to retrieve relevant information that a language model\n",
      "can then use to generate more informed and accurate responses.  ### Application\n",
      "in RAG  In a RAG setup, the vector store plays a crucial role in the retrieval\n",
      "phase. When a query is made, the system retrieves relevant vectors from the\n",
      "vector store that are similar to the query vector. These retrieved vectors\n",
      "contain information that can be used to augment the generation process, allowing\n",
      "the language model to produce responses that are not only coherent but also\n",
      "enriched with factual and contextually relevant information.  ### Conclusion  In\n",
      "summary, a RAG vector store is a powerful tool in the realm of NLP, enabling\n",
      "systems to leverage large datasets effectively by combining retrieval and\n",
      "generation capabilities. By storing and managing vectorized data points, it\n",
      "facilitates efficient similarity searches and enhances the quality of generated\n",
      "text through retrieval-augmented techniques.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_response = call_llm_with_full_txt(augmented_input)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed639b-0303-4f03-8849-b2c50839c3a1",
   "metadata": {},
   "source": [
    "### Advanced RAG:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9fc4f7-8a3a-4d59-a985-ee7a5d017cb5",
   "metadata": {},
   "source": [
    "## Vector Search:\n",
    "\n",
    "The metrics are the same for both similarity methods as for naïve RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3777e54a-337a-43d2-8dbd-b1e7018fba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(text_input, records):\n",
    "    best_score =0\n",
    "    best_record=None\n",
    "    for record in records:\n",
    "        current_score = calculate_cosine_similarity(text_input,record)\n",
    "        if current_score > best_score:\n",
    "            best_score= current_score\n",
    "            best_record= record\n",
    "\n",
    "        return best_score, best_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a50fb038-1b02-4fe8-9f8a-59ae634de929",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_similarity_score, best_matching_record = find_best_match(query, db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9efb91cb-823d-4114-9016-0b61eba89a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach\n",
      "in the field of artificial intelligence, particularly within the realm of\n",
      "natural language processing (NLP).\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(best_matching_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "508b0b71-e1c7-4416-9a15-5831db8f81a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04182819438170016"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e8516e8-73a6-46bd-9962-9a5b8cb3112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3785786642252672"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_enhanced_similarity(query, best_matching_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae60886-5f5d-4753-841b-7f3a1298cdd1",
   "metadata": {},
   "source": [
    "### Augmented input:\n",
    "Let’s now augment the user query with this information retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fce6817c-d1b4-48b0-b4f4-530800e535a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_input_= query+ \":\"+ best_matching_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a5f468e-8092-4e07-b755-d0739aba3300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "define a rag store:Retrieval Augmented Generation (RAG) represents a\n",
      "sophisticated hybrid approach in the field of artificial intelligence,\n",
      "particularly within the realm of natural language processing (NLP).\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(augmented_input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f2237-54cf-4782-ba7b-fa55db041914",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02711811-b9c7-47b4-b516-1d7fa36fdb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is an advanced technique in artificial intelligence, specifically within the domain of natural language processing (NLP). It combines two key components: retrieval and generation, to enhance the performance of language models.\\n\\n1. **Retrieval Component**: This part of RAG involves searching for and retrieving relevant information from a large corpus or database. When a query or prompt is given, the retrieval system scans through a vast amount of data to find documents or pieces of information that are most relevant to the query. This is akin to how search engines work, where they pull up the most pertinent web pages based on a user\\'s search terms.\\n\\n2. **Augmented Generation Component**: Once the relevant information is retrieved, the generation component comes into play. This involves using a language model to generate a response or output that is informed by the retrieved information. The generation process is \"augmented\" because it is not solely based on the model\\'s pre-existing knowledge but is enhanced by the newly retrieved data. This allows for more accurate, contextually relevant, and up-to-date responses.\\n\\n3. **Hybrid Approach**: RAG is considered a hybrid approach because it effectively combines the strengths of both retrieval and generation. Traditional language models rely heavily on pre-trained data, which can become outdated or lack specific information. By integrating a retrieval mechanism, RAG can access the latest and most relevant data, ensuring that the generated responses are both informed and current.\\n\\n4. **Applications in NLP**: RAG is particularly useful in applications where accuracy and relevance are crucial, such as question-answering systems, chatbots, and information retrieval tasks. It allows these systems to provide more precise answers by leveraging external knowledge sources, thus overcoming some of the limitations of standalone generative models.\\n\\nOverall, Retrieval-Augmented Generation represents a significant advancement in NLP, offering a more dynamic and informed approach to generating language-based outputs.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_llm_with_full_txt(augmented_input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "635bb69a-a08e-4b31-b46f-5b68d6e61ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "Retrieval-Augmented Generation (RAG) is an advanced technique in artificial\n",
      "intelligence, specifically within the domain of natural language processing\n",
      "(NLP). It combines two key components: retrieval and generation, to enhance the\n",
      "performance of language models.  1. **Retrieval Component**: This part of RAG\n",
      "involves searching through a large corpus of documents or data to find relevant\n",
      "information that can aid in generating a more accurate and contextually\n",
      "appropriate response. The retrieval process is akin to how search engines work,\n",
      "where the system identifies and extracts pertinent information from a vast\n",
      "database based on the input query.  2. **Generation Component**: Once the\n",
      "relevant information is retrieved, the generation component takes over. This\n",
      "involves using a language model to generate a coherent and contextually relevant\n",
      "response or output. The generation process leverages the retrieved information\n",
      "to produce more informed and precise responses than what might be possible with\n",
      "a standalone generative model.  3. **Hybrid Approach**: The hybrid nature of RAG\n",
      "lies in its ability to combine the strengths of both retrieval and generation.\n",
      "By integrating these two processes, RAG can produce outputs that are not only\n",
      "contextually rich but also grounded in factual information. This is particularly\n",
      "useful in scenarios where the model needs to provide detailed, accurate, and up-\n",
      "to-date information.  4. **Applications in NLP**: RAG is particularly beneficial\n",
      "in applications such as question answering, conversational agents, and content\n",
      "creation, where the need for accurate and contextually relevant information is\n",
      "paramount. It allows systems to handle complex queries by leveraging external\n",
      "knowledge sources, thereby improving the quality and reliability of the\n",
      "generated responses.  Overall, Retrieval-Augmented Generation represents a\n",
      "significant advancement in NLP, offering a more robust and effective way to\n",
      "handle complex language tasks by combining the best of both retrieval and\n",
      "generative capabilities.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(call_llm_with_full_txt(augmented_input_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ef1d7-a697-4289-bb1c-1ef4855bd030",
   "metadata": {},
   "source": [
    "## Index-Based Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a72f9b0b-8223-41ca-aa13-7ade98ed0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def set_up_vectorizer(records):\n",
    "    vectorizer=TfidfVectorizer()\n",
    "    tfidf_matrix= vectorizer.fit_transform(records)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def find_best_match(query, vectorizer, tfidf_matrix):\n",
    "    query_tfidf= vectorizer.transform([query])\n",
    "    similarities= cosine_similarity(query_tfidf, tfidf_matrix)\n",
    "    best_index= similarities.argmax()\n",
    "    best_score= similarities[0, best_index]\n",
    "    return best_score, best_index\n",
    "\n",
    "\n",
    "\n",
    "vectorizer, tfidf_matrix= set_up_vectorizer(db_records)\n",
    "best_similarity_score, best_index = find_best_match(query, vectorizer, tfidf_matrix)\n",
    "best_matching_record = db_records[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7b58901-df49-46b4-b0b6-a1b16b543334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "A RAG vector store is a database or dataset that contains vectorized data\n",
      "points.\n",
      "-------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.40746671865845496, 27)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_formatted_response(best_matching_record)\n",
    "find_best_match(query, vectorizer, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ee81e-c06b-4d2b-a5b8-b160dc244bb0",
   "metadata": {},
   "source": [
    "### Augmented input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e66e1c53-aefe-4d02-9f7f-1bbea245bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "define a rag store: A RAG vector store is a database or dataset that contains\n",
      "vectorized data points.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_input=query+\": \"+best_matching_record\n",
    "print_formatted_response(augmented_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1ea94cf-f90a-4776-b6e8-49ee97dadb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "Certainly! Let's break down the concept of a RAG vector store and understand its\n",
      "components in detail.  ### RAG (Retrieval-Augmented Generation)  RAG stands for\n",
      "Retrieval-Augmented Generation, which is a framework used in natural language\n",
      "processing (NLP) to enhance the generation of text by incorporating external\n",
      "information retrieval. The idea is to improve the quality and relevance of\n",
      "generated content by retrieving relevant information from a large dataset or\n",
      "database before generating the final output. This approach is particularly\n",
      "useful in tasks where the model needs to generate responses based on a vast\n",
      "amount of information, such as question answering or conversational agents.  ###\n",
      "Vector Store  A vector store is a specialized type of database or dataset that\n",
      "stores data in the form of vectors. In the context of machine learning and NLP,\n",
      "vectors are numerical representations of data points. These vectors are often\n",
      "generated through a process called \"embedding,\" where raw data (such as words,\n",
      "sentences, or documents) is transformed into a fixed-size numerical format that\n",
      "captures the semantic meaning of the data.  ### Components of a RAG Vector Store\n",
      "1. **Vectorization**: The first step in creating a vector store is to convert\n",
      "raw data into vectors. This is typically done using embedding models like\n",
      "Word2Vec, GloVe, BERT, or other transformer-based models. These models map data\n",
      "into a high-dimensional space where semantically similar items are closer\n",
      "together.  2. **Storage**: Once the data is vectorized, it is stored in a\n",
      "database designed to handle high-dimensional vectors efficiently. This storage\n",
      "system must support fast retrieval operations, as the RAG framework relies on\n",
      "quickly finding relevant vectors during the retrieval phase.  3. **Retrieval**:\n",
      "The retrieval component of a RAG system involves searching the vector store to\n",
      "find vectors that are most similar to a given query vector. This is often done\n",
      "using similarity measures like cosine similarity or Euclidean distance. The\n",
      "retrieved vectors represent the most relevant pieces of information that can be\n",
      "used to augment the generation process.  4. **Augmentation**: After retrieving\n",
      "relevant vectors, the information they contain is used to inform or guide the\n",
      "generation process. This can involve concatenating retrieved text with the input\n",
      "query, using retrieved data as context, or incorporating it into the model's\n",
      "attention mechanism.  5. **Generation**: Finally, the augmented information is\n",
      "used to generate the desired output. This step typically involves a generative\n",
      "model, such as a transformer-based language model, which produces the final text\n",
      "output based on both the input query and the retrieved information.  ###\n",
      "Benefits of a RAG Vector Store  - **Improved Relevance**: By incorporating\n",
      "external information, RAG systems can generate more accurate and contextually\n",
      "relevant responses. - **Scalability**: Vector stores can handle large datasets,\n",
      "making it feasible to retrieve information from vast corpora. - **Flexibility**:\n",
      "The framework can be adapted to various tasks, including question answering,\n",
      "summarization, and dialogue systems.  In summary, a RAG vector store is a\n",
      "crucial component of the Retrieval-Augmented Generation framework, enabling the\n",
      "efficient storage and retrieval of vectorized data to enhance the quality of\n",
      "generated text.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_formatted_response(call_llm_with_full_txt(augmented_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90736240-5e48-41d0-872e-5aab6508a1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(),\n",
       " <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       " \twith 630 stored elements and shape (28, 297)>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "set_up_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64733216-0b48-49a6-b458-900ac3c0b936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'access', 'accuracy', 'accurate', 'adapt', 'additional',\n",
       "       'advancement', 'advice', 'algorithms', 'allows', 'also', 'amounts',\n",
       "       'and', 'answers', 'approach', 'appropriate', 'architectures',\n",
       "       'are', 'areas', 'artificial', 'as', 'associated', 'at',\n",
       "       'augmented', 'available', 'based', 'be', 'behind', 'benefits',\n",
       "       'bert', 'best', 'biases', 'bidirectional', 'both', 'built', 'but',\n",
       "       'by', 'can', 'capabilities', 'capable', 'challenge', 'challenges',\n",
       "       'coherent', 'combines', 'comes', 'complementing', 'complex',\n",
       "       'complexity', 'component', 'comprehend', 'computational',\n",
       "       'contained', 'contains', 'content', 'context', 'contextually',\n",
       "       'core', 'corpus', 'correct', 'correctness', 'create', 'crucial',\n",
       "       'current', 'data', 'database', 'databases', 'dataset', 'date',\n",
       "       'deeply', 'depth', 'detailed', 'details', 'determination',\n",
       "       'determined', 'direct', 'directly', 'documents', 'does', 'domains',\n",
       "       'dynamically', 'each', 'effectively', 'embeddings', 'encoder',\n",
       "       'enhance', 'enhancing', 'ensures', 'ensuring', 'errors',\n",
       "       'external', 'facts', 'factual', 'fed', 'fetch', 'fetched', 'field',\n",
       "       'final', 'find', 'first', 'fluency', 'fluent', 'for', 'form',\n",
       "       'frequent', 'from', 'furthermore', 'generate', 'generates',\n",
       "       'generating', 'generation', 'generative', 'given', 'gpt',\n",
       "       'grammatically', 'guided', 'happens', 'high', 'hybrid',\n",
       "       'implementations', 'in', 'include', 'incorporate', 'incorporated',\n",
       "       'incorporating', 'influenced', 'information', 'informative',\n",
       "       'innovatively', 'input', 'integrating', 'integration',\n",
       "       'integrator', 'intelligence', 'interpret', 'into', 'introducing',\n",
       "       'is', 'it', 'its', 'keyword', 'knowledge', 'language', 'large',\n",
       "       'latest', 'level', 'leverages', 'like', 'linguistic',\n",
       "       'maintaining', 'making', 'managing', 'matches', 'matching',\n",
       "       'meaning', 'measures', 'mechanism', 'medical', 'merges', 'merging',\n",
       "       'methodology', 'might', 'mixture', 'model', 'models', 'more',\n",
       "       'natural', 'need', 'needing', 'network', 'neural', 'new', 'nlp',\n",
       "       'not', 'now', 'nuanced', 'nuances', 'of', 'offers', 'on', 'only',\n",
       "       'or', 'other', 'output', 'outputs', 'overhead', 'part',\n",
       "       'particularly', 'pieces', 'points', 'potential', 'pre',\n",
       "       'precision', 'process', 'processed', 'processes', 'processing',\n",
       "       'produce', 'producing', 'prompt', 'prompts', 'provide', 'quality',\n",
       "       'query', 'rag', 'range', 'raw', 'real', 'realm', 'received',\n",
       "       'relevance', 'relevant', 'remain', 'remains', 'representations',\n",
       "       'represents', 'require', 'requirement', 'response', 'responses',\n",
       "       'responsible', 'retraining', 'retrievable', 'retrieval',\n",
       "       'retrieve', 'retrieved', 'rich', 'seamlessly', 'search',\n",
       "       'searches', 'semantic', 'significant', 'similar', 'similarity',\n",
       "       'simple', 'since', 'snippets', 'solely', 'some', 'sophisticated',\n",
       "       'sources', 'specific', 'standalone', 'store', 'strengths',\n",
       "       'substantial', 'summary', 'support', 'synthesizes', 'system',\n",
       "       'systems', 'tackle', 'tailored', 'tasks', 'technical',\n",
       "       'techniques', 'technologies', 'text', 'texts', 'textual', 'that',\n",
       "       'the', 'their', 'them', 'then', 'these', 'they', 'this', 'those',\n",
       "       'through', 'time', 'to', 'token', 'trained', 'training',\n",
       "       'transformer', 'transformers', 'trends', 'typically', 'understand',\n",
       "       'up', 'used', 'useful', 'uses', 'usually', 'utilize', 'various',\n",
       "       'vast', 'vector', 'vectorized', 'when', 'where', 'which', 'while',\n",
       "       'wide', 'with', 'within', 'without'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(db_records)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "34cae465-4aab-4701-8628-137eb4bb79ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ba9c9c1-0fc2-49ca-b013-eaed961cca00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "020ecdcc-bda4-4193-b7fd-ab693378683f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 297)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7af9154-a06f-4d31-9d3b-f59c3af76e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "abd18299-bffb-4442-8fa7-652724c6527c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 630 stored elements and shape (28, 297)>\n",
      "  Coords\tValues\n",
      "  (0, 222)\t0.10091348627954093\n",
      "  (0, 23)\t0.1539082835028258\n",
      "  (0, 109)\t0.1464357841118198\n",
      "  (0, 203)\t0.16226182711181428\n",
      "  (0, 214)\t0.23182532487993252\n",
      "  (0, 238)\t0.26058218654196424\n",
      "  (0, 117)\t0.26058218654196424\n",
      "  (0, 14)\t0.26058218654196424\n",
      "  (0, 119)\t0.1464357841118198\n",
      "  (0, 260)\t0.16868957146941504\n",
      "  (0, 95)\t0.23182532487993252\n",
      "  (0, 176)\t0.2262032947934785\n",
      "  (0, 19)\t0.23182532487993252\n",
      "  (0, 132)\t0.23182532487993252\n",
      "  (0, 186)\t0.23182532487993252\n",
      "  (0, 295)\t0.26058218654196424\n",
      "  (0, 207)\t0.26058218654196424\n",
      "  (0, 165)\t0.23182532487993252\n",
      "  (0, 141)\t0.13967608617281693\n",
      "  (0, 195)\t0.26058218654196424\n",
      "  (0, 171)\t0.26058218654196424\n",
      "  (1, 222)\t0.09418296782117092\n",
      "  (1, 109)\t0.136669113825548\n",
      "  (1, 260)\t0.15743866421838287\n",
      "  (1, 176)\t0.21111645648195462\n",
      "  :\t:\n",
      "  (26, 245)\t0.17332710410898458\n",
      "  (26, 6)\t0.17332710410898458\n",
      "  (26, 158)\t0.17332710410898458\n",
      "  (26, 255)\t0.17332710410898458\n",
      "  (26, 60)\t0.17332710410898458\n",
      "  (26, 68)\t0.17332710410898458\n",
      "  (26, 49)\t0.17332710410898458\n",
      "  (26, 284)\t0.17332710410898458\n",
      "  (26, 286)\t0.17332710410898458\n",
      "  (26, 11)\t0.17332710410898458\n",
      "  (26, 24)\t0.17332710410898458\n",
      "  (26, 258)\t0.17332710410898458\n",
      "  (26, 102)\t0.17332710410898458\n",
      "  (27, 203)\t0.21538197433632383\n",
      "  (27, 259)\t0.21538197433632383\n",
      "  (27, 136)\t0.19437491156658515\n",
      "  (27, 180)\t0.20429370578152264\n",
      "  (27, 64)\t0.2596288086428521\n",
      "  (27, 63)\t0.2596288086428521\n",
      "  (27, 287)\t0.30771868567332367\n",
      "  (27, 242)\t0.3458897684888577\n",
      "  (27, 66)\t0.3458897684888577\n",
      "  (27, 52)\t0.3458897684888577\n",
      "  (27, 288)\t0.3458897684888577\n",
      "  (27, 188)\t0.3458897684888577\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "859f56bf-5163-49db-a9a5-579c060c5458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 630 stored elements and shape (28, 297)>\n",
      "  Coords\tValues\n",
      "  (0, 222)\t0.10091348627954093\n",
      "  (0, 23)\t0.1539082835028258\n",
      "  (0, 109)\t0.1464357841118198\n",
      "  (0, 203)\t0.16226182711181428\n",
      "  (0, 214)\t0.23182532487993252\n",
      "  (0, 238)\t0.26058218654196424\n",
      "  (0, 117)\t0.26058218654196424\n",
      "  (0, 14)\t0.26058218654196424\n",
      "  (0, 119)\t0.1464357841118198\n",
      "  (0, 260)\t0.16868957146941504\n",
      "  (0, 95)\t0.23182532487993252\n",
      "  (0, 176)\t0.2262032947934785\n",
      "  (0, 19)\t0.23182532487993252\n",
      "  (0, 132)\t0.23182532487993252\n",
      "  (0, 186)\t0.23182532487993252\n",
      "  (0, 295)\t0.26058218654196424\n",
      "  (0, 207)\t0.26058218654196424\n",
      "  (0, 165)\t0.23182532487993252\n",
      "  (0, 141)\t0.13967608617281693\n",
      "  (0, 195)\t0.26058218654196424\n",
      "  (0, 171)\t0.26058218654196424\n",
      "  (1, 222)\t0.09418296782117092\n",
      "  (1, 109)\t0.136669113825548\n",
      "  (1, 260)\t0.15743866421838287\n",
      "  (1, 176)\t0.21111645648195462\n",
      "  :\t:\n",
      "  (26, 245)\t0.17332710410898458\n",
      "  (26, 6)\t0.17332710410898458\n",
      "  (26, 158)\t0.17332710410898458\n",
      "  (26, 255)\t0.17332710410898458\n",
      "  (26, 60)\t0.17332710410898458\n",
      "  (26, 68)\t0.17332710410898458\n",
      "  (26, 49)\t0.17332710410898458\n",
      "  (26, 284)\t0.17332710410898458\n",
      "  (26, 286)\t0.17332710410898458\n",
      "  (26, 11)\t0.17332710410898458\n",
      "  (26, 24)\t0.17332710410898458\n",
      "  (26, 258)\t0.17332710410898458\n",
      "  (26, 102)\t0.17332710410898458\n",
      "  (27, 203)\t0.21538197433632383\n",
      "  (27, 259)\t0.21538197433632383\n",
      "  (27, 136)\t0.19437491156658515\n",
      "  (27, 180)\t0.20429370578152264\n",
      "  (27, 64)\t0.2596288086428521\n",
      "  (27, 63)\t0.2596288086428521\n",
      "  (27, 287)\t0.30771868567332367\n",
      "  (27, 242)\t0.3458897684888577\n",
      "  (27, 66)\t0.3458897684888577\n",
      "  (27, 52)\t0.3458897684888577\n",
      "  (27, 288)\t0.3458897684888577\n",
      "  (27, 188)\t0.3458897684888577\n"
     ]
    }
   ],
   "source": [
    "set_up_vectorizer(db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cfeb85-5f23-48cb-abdc-1075fff604b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d5f2e-943f-4cf9-b6ec-724f640b905c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26271896-c2a3-4c2e-9a2d-845fb7028468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c9915-bc7e-4fe3-85fb-fb5df5e6be89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fe669-1bc4-473d-a07f-8346cde04347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ab885-5871-4280-bf0b-f56c5c23292d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2a3d1-21bb-4282-8676-d20bf8b510bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73ab1a-0086-4bc5-a394-96da1e3a2517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9ca16-241d-436f-9d3b-07727264877f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf9c4e-cd51-4c38-8f3b-cbbf5515567b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e3bc0-0fb1-4d20-b1bf-52bbd137bf8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4248a99-05ad-4d11-bece-95ef8a4e2e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b74c2-1d35-47b4-88ed-7f4b9aeb6b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc01fd-41a8-40fd-bf35-9508bd16efec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb0ae2-2c9a-4b15-b9ca-30da115e27f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ea915-a732-4bfb-9eba-62e595cc0a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1bddfe-78b1-4250-9c17-e8fdd84f3b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee265a-8eab-41ac-b064-193dae4af617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2af1d2-6c50-4be6-9e9e-971b3c6a10de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656a93e-7bd1-4987-8d4f-982e48323b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b358c-fc48-4f88-b3d6-b3f2205f40dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
