{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "516fac83-b0fa-49f3-a5a2-1ab8d8f95ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "934b10c6-8d21-4f80-8eac-b233a6e9f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai==1.40.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9e8046f-f92a-4c64-9354-53d3167b291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('api_key.txt','r') as key:\n",
    "    API_KEY = key.readline().strip()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = API_KEY # we define the env varibale to be accessible for all scripts not just the currently one.\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5709f52-5c29-4fa0-8c7c-b31169084e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "gptmodel='gpt-4o'\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7eb639d5-2eb9-472d-8035-6a197eaa9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which creates a prompt\n",
    "def call_llm_with_full_txt(txt):\n",
    "    txt_input= textwrap.fill(txt,width=30)\n",
    "    prompt = f\"Please elaborate on the following content:\\n{txt_input}\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model= gptmodel,\n",
    "            messages=[{'role':'system', 'content':\"You are an expert Natural Language Processing exercise expert.\"},\n",
    "                  {\"role\": \"assistant\", \"content\": \"1.You can explain read the input and answer in detail\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1)\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e :\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee32c7d7-783b-4c41-a414-ea8247d9afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_response(response):\n",
    "    wrapper = textwrap.TextWrapper(width=80)\n",
    "    wrap_res = wrapper.fill(text=response)\n",
    "    print(\"Response:\")\n",
    "    print(\"---------\")\n",
    "    print(wrap_res)\n",
    "    print(\"-------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47071950-d5c3-4a27-ae61-8174390f205d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b643539-bed2-4106-ab85-d4c5b1ebbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records = [\n",
    "    \"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).\",\n",
    "    \"It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.\",\n",
    "    \"This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.\",\n",
    "    \"At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).\",\n",
    "    \"This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.\",\n",
    "    \"Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.\",\n",
    "    \"This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.\",\n",
    "    \"The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.\",\n",
    "    \"This component merges the outputs from the language model and the retrieval system.\",\n",
    "    \"It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.\",\n",
    "    \"The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.\",\n",
    "    \"When a query or prompt is received, the system first processes it to understand the requirement or the context.\",\n",
    "    \"Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.\",\n",
    "    \"This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.\",\n",
    "    \"The retrieved documents are then fed into the language model.\",\n",
    "    \"In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.\",\n",
    "    \"The language model, now augmented with direct access to retrieved information, generates a response.\",\n",
    "    \"This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.\",\n",
    "    \"By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.\",\n",
    "    \"This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.\",\n",
    "    \"Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.\",\n",
    "    \"This allows them to remain current with the latest knowledge and trends without needing frequent retraining.\",\n",
    "    \"With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.\",\n",
    "    \"While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.\",\n",
    "    \"These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.\",\n",
    "    \"Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.\",\n",
    "    \"In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.\",\n",
    "    \"A RAG vector store is a database or dataset that contains vectorized data points.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bd97d479-675a-4e79-b024-8ce844ac79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_records_join = ''.join(db_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "763043ad-9a2b-4a08-a01c-15f2420a2ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Retrieval Augmented Generation (RAG) represents a sophisticated hybrid approach in the field of artificial intelligence, particularly within the realm of natural language processing (NLP).It innovatively combines the capabilities of neural network-based language models with retrieval systems to enhance the generation of text, making it more accurate, informative, and contextually relevant.This methodology leverages the strengths of both generative and retrieval architectures to tackle complex tasks that require not only linguistic fluency but also factual correctness and depth of knowledge.At the core of Retrieval Augmented Generation (RAG) is a generative model, typically a transformer-based neural network, similar to those used in models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers).This component is responsible for producing coherent and contextually appropriate language outputs based on a mixture of input prompts and additional information fetched by the retrieval component.Complementing the language model is the retrieval system, which is usually built on a database of documents or a corpus of texts.This system uses techniques from information retrieval to find and fetch documents that are relevant to the input query or prompt.The mechanism of relevance determination can range from simple keyword matching to more complex semantic search algorithms which interpret the meaning behind the query to find the best matches.This component merges the outputs from the language model and the retrieval system.It effectively synthesizes the raw data fetched by the retrieval system into the generative process of the language model.The integrator ensures that the information from the retrieval system is seamlessly incorporated into the final text output, enhancing the model's ability to generate responses that are not only fluent and grammatically correct but also rich in factual details and context-specific nuances.When a query or prompt is received, the system first processes it to understand the requirement or the context.Based on the processed query, the retrieval system searches through its database to find relevant documents or information snippets.This retrieval is guided by the similarity of content in the documents to the query, which can be determined through various techniques like vector embeddings or semantic similarity measures.The retrieved documents are then fed into the language model.In some implementations, this integration happens at the token level, where the model can access and incorporate specific pieces of information from the retrieved texts dynamically as it generates each part of the response.The language model, now augmented with direct access to retrieved information, generates a response.This response is not only influenced by the training of the model but also by the specific facts and details contained in the retrieved documents, making it more tailored and accurate.By directly incorporating information from external sources, Retrieval Augmented Generation (RAG) models can produce responses that are more factual and relevant to the given query.This is particularly useful in domains like medical advice, technical support, and other areas where precision and up-to-date knowledge are crucial.Retrieval Augmented Generation (RAG) systems can dynamically adapt to new information since they retrieve data in real-time from their databases.This allows them to remain current with the latest knowledge and trends without needing frequent retraining.With access to a wide range of documents, Retrieval Augmented Generation (RAG) systems can provide detailed and nuanced answers that a standalone language model might not be capable of generating based solely on its pre-trained knowledge.While Retrieval Augmented Generation (RAG) offers substantial benefits, it also comes with its challenges.These include the complexity of integrating retrieval and generation systems, the computational overhead associated with real-time data retrieval, and the need for maintaining a large, up-to-date, and high-quality database of retrievable texts.Furthermore, ensuring the relevance and accuracy of the retrieved information remains a significant challenge, as does managing the potential for introducing biases or errors from the external sources.In summary, Retrieval Augmented Generation represents a significant advancement in the field of artificial intelligence, merging the best of retrieval-based and generative technologies to create systems that not only understand and generate natural language but also deeply comprehend and utilize the vast amounts of information available in textual form.A RAG vector store is a database or dataset that contains vectorized data points.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_records_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d2ab7c1-9fad-490f-b66a-e6b13da69999",
   "metadata": {},
   "outputs": [],
   "source": [
    "wraped_db_records = textwrap.fill(db_records_join, width=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0420f06b-fdd4-468e-b6e7-d4e21f32ca09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Retrieval Augmented Generation\\n(RAG) represents a\\nsophisticated hybrid approach\\nin the field of artificial\\nintelligence, particularly\\nwithin the realm of natural\\nlanguage processing (NLP).It\\ninnovatively combines the\\ncapabilities of neural\\nnetwork-based language models\\nwith retrieval systems to\\nenhance the generation of\\ntext, making it more accurate,\\ninformative, and contextually\\nrelevant.This methodology\\nleverages the strengths of\\nboth generative and retrieval\\narchitectures to tackle\\ncomplex tasks that require not\\nonly linguistic fluency but\\nalso factual correctness and\\ndepth of knowledge.At the core\\nof Retrieval Augmented\\nGeneration (RAG) is a\\ngenerative model, typically a\\ntransformer-based neural\\nnetwork, similar to those used\\nin models like GPT (Generative\\nPre-trained Transformer) or\\nBERT (Bidirectional Encoder\\nRepresentations from\\nTransformers).This component\\nis responsible for producing\\ncoherent and contextually\\nappropriate language outputs\\nbased on a mixture of input\\nprompts and additional\\ninformation fetched by the\\nretrieval\\ncomponent.Complementing the\\nlanguage model is the\\nretrieval system, which is\\nusually built on a database of\\ndocuments or a corpus of\\ntexts.This system uses\\ntechniques from information\\nretrieval to find and fetch\\ndocuments that are relevant to\\nthe input query or prompt.The\\nmechanism of relevance\\ndetermination can range from\\nsimple keyword matching to\\nmore complex semantic search\\nalgorithms which interpret the\\nmeaning behind the query to\\nfind the best matches.This\\ncomponent merges the outputs\\nfrom the language model and\\nthe retrieval system.It\\neffectively synthesizes the\\nraw data fetched by the\\nretrieval system into the\\ngenerative process of the\\nlanguage model.The integrator\\nensures that the information\\nfrom the retrieval system is\\nseamlessly incorporated into\\nthe final text output,\\nenhancing the model's ability\\nto generate responses that are\\nnot only fluent and\\ngrammatically correct but also\\nrich in factual details and\\ncontext-specific nuances.When\\na query or prompt is received,\\nthe system first processes it\\nto understand the requirement\\nor the context.Based on the\\nprocessed query, the retrieval\\nsystem searches through its\\ndatabase to find relevant\\ndocuments or information\\nsnippets.This retrieval is\\nguided by the similarity of\\ncontent in the documents to\\nthe query, which can be\\ndetermined through various\\ntechniques like vector\\nembeddings or semantic\\nsimilarity measures.The\\nretrieved documents are then\\nfed into the language model.In\\nsome implementations, this\\nintegration happens at the\\ntoken level, where the model\\ncan access and incorporate\\nspecific pieces of information\\nfrom the retrieved texts\\ndynamically as it generates\\neach part of the response.The\\nlanguage model, now augmented\\nwith direct access to\\nretrieved information,\\ngenerates a response.This\\nresponse is not only\\ninfluenced by the training of\\nthe model but also by the\\nspecific facts and details\\ncontained in the retrieved\\ndocuments, making it more\\ntailored and accurate.By\\ndirectly incorporating\\ninformation from external\\nsources, Retrieval Augmented\\nGeneration (RAG) models can\\nproduce responses that are\\nmore factual and relevant to\\nthe given query.This is\\nparticularly useful in domains\\nlike medical advice, technical\\nsupport, and other areas where\\nprecision and up-to-date\\nknowledge are\\ncrucial.Retrieval Augmented\\nGeneration (RAG) systems can\\ndynamically adapt to new\\ninformation since they\\nretrieve data in real-time\\nfrom their databases.This\\nallows them to remain current\\nwith the latest knowledge and\\ntrends without needing\\nfrequent retraining.With\\naccess to a wide range of\\ndocuments, Retrieval Augmented\\nGeneration (RAG) systems can\\nprovide detailed and nuanced\\nanswers that a standalone\\nlanguage model might not be\\ncapable of generating based\\nsolely on its pre-trained\\nknowledge.While Retrieval\\nAugmented Generation (RAG)\\noffers substantial benefits,\\nit also comes with its\\nchallenges.These include the\\ncomplexity of integrating\\nretrieval and generation\\nsystems, the computational\\noverhead associated with real-\\ntime data retrieval, and the\\nneed for maintaining a large,\\nup-to-date, and high-quality\\ndatabase of retrievable\\ntexts.Furthermore, ensuring\\nthe relevance and accuracy of\\nthe retrieved information\\nremains a significant\\nchallenge, as does managing\\nthe potential for introducing\\nbiases or errors from the\\nexternal sources.In summary,\\nRetrieval Augmented Generation\\nrepresents a significant\\nadvancement in the field of\\nartificial intelligence,\\nmerging the best of retrieval-\\nbased and generative\\ntechnologies to create systems\\nthat not only understand and\\ngenerate natural language but\\nalso deeply comprehend and\\nutilize the vast amounts of\\ninformation available in\\ntextual form.A RAG vector\\nstore is a database or dataset\\nthat contains vectorized data\\npoints.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wraped_db_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb33ccdd-e91d-4e13-a7df-cfcbfc6a0d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'define a rag store'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67b1ab9b-3755-4595-9f1b-dd23120b1fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "---------\n",
      "A \"rag store\" traditionally refers to a type of business or shop that deals in\n",
      "the collection, sorting, and resale of used textiles and clothing. These\n",
      "establishments have historically played a role in the recycling and repurposing\n",
      "of fabric materials. Here's a more detailed explanation of what a rag store\n",
      "typically involves:  1. **Collection**: Rag stores often collect used clothing\n",
      "and textiles from various sources. This can include donations from individuals,\n",
      "collections from thrift stores, or purchasing bulk textiles from other\n",
      "businesses. The goal is to gather a wide variety of materials that can be\n",
      "processed and resold.  2. **Sorting**: Once the textiles are collected, they are\n",
      "sorted based on several criteria such as material type, quality, and potential\n",
      "for reuse. This sorting process is crucial as it determines the next steps for\n",
      "each item. For example, high-quality clothing might be set aside for resale as\n",
      "second-hand apparel, while lower-quality items might be designated for recycling\n",
      "or repurposing.  3. **Resale and Repurposing**: After sorting, the textiles are\n",
      "either resold or repurposed. Resale involves selling the items as second-hand\n",
      "clothing, often at a lower price than new garments. Repurposing can involve\n",
      "transforming the textiles into new products, such as cleaning rags, insulation\n",
      "materials, or even raw materials for new textile production.  4. **Recycling**:\n",
      "Items that cannot be resold or repurposed are often sent for recycling. This\n",
      "process involves breaking down the textiles into fibers, which can then be used\n",
      "to create new fabrics or other products. Recycling helps reduce waste and\n",
      "supports sustainable practices in the textile industry.  5. **Environmental\n",
      "Impact**: Rag stores contribute to environmental sustainability by reducing the\n",
      "amount of textile waste that ends up in landfills. By promoting the reuse and\n",
      "recycling of textiles, these stores help conserve resources and reduce the\n",
      "environmental footprint of the fashion and textile industries.  Overall, rag\n",
      "stores serve as an important link in the circular economy of textiles, promoting\n",
      "sustainability and resource efficiency. They provide an alternative to the fast\n",
      "fashion model by extending the life cycle of clothing and textiles.\n",
      "-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_response = call_llm_with_full_txt(query)\n",
    "print_formatted_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d932b-0b27-4e57-9f6e-7b365f84f6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77dfaa-8b63-4a22-ac3b-3b040f2f6f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00be48a-80c2-4e28-bad7-cb1f54fe48b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760da414-ff73-4e5d-b06c-de7842a4ee4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f1bc0-bf35-4466-8824-67a4222b1469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5fabc8-a0d9-4d62-8836-a91f75518b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e6842-2650-45ae-8423-7543b7e4213f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359a7c4-3569-4063-a885-851de044d9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323ccf4-1920-46d9-af6d-ab5879c9d791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6817c-d1b4-48b0-b4f4-530800e535a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
